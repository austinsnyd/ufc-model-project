---
title: "UFC_RF"
author: "Austin Snyder"
date: "`r Sys.Date()`"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(randomForest)
library(caret)
library(dplyr)
```

## Import Data
```{r}
# Load dataset with all features
ufc_rf_data <- readRDS("ufc_men_imp.rds")

# Remove non-informative columns and reassign to ufc_rf_data
ufc_rf_data <- ufc_rf_data %>% select(-Date, -Location, -Country)


```




```{r}
# Select only numeric features
numeric_features <- names(ufc_rf_data)[sapply(ufc_rf_data, is.numeric)]

# Compute correlation matrix
cor_matrix <- cor(ufc_rf_data[numeric_features], use = "complete.obs")

# Find highly correlated feature pairs (above 0.85)
high_cor_pairs <- which(abs(cor_matrix) > 0.80 & lower.tri(cor_matrix), arr.ind = TRUE)

# Print highly correlated variables
print(high_cor_pairs)

# Example: If `RedReachCms` is highly correlated with `ReachDif`, remove `RedReachCms`
features_to_remove <- c("RedReachCms")  # Adjust based on Step 1 results

# Drop from dataset
ufc_rf_data <- ufc_rf_data %>% select(-all_of(features_to_remove))

# Confirm columns were removed
names(ufc_rf_data)

```


```{r}

# Remove fighter names before training (not predictive)
ufc_rf_data <- ufc_rf_data %>% select(-RedFighter, -BlueFighter)

# Train a preliminary Random Forest model using all features
set.seed(1616)
rf_full <- randomForest(Winner ~ ., data = ufc_rf_data, ntree = 500, importance = TRUE)

# View summary
print(rf_full)

# View feature importance
importance(rf_full)
varImpPlot(rf_full, main = "Feature Importance - Random Forest")

```

```{r}
# Extract and sort variable importance
importance_df <- data.frame(Feature = rownames(importance(rf_full)), 
                            MeanDecreaseGini = importance(rf_full)[, "MeanDecreaseGini"])
importance_df <- importance_df[order(-importance_df$MeanDecreaseGini), ]
head(importance_df, 15)

```


```{r}
# Select the top N most important features (adjust as needed)
top_features <- head(importance_df$Feature, 15)  # Keep top 15

# Subset train and test data
train_data_rf <- train_data %>% select(all_of(top_features), Winner)
test_data_rf <- test_data %>% select(all_of(top_features), Winner)

# Confirm structure
str(train_data_rf)

```


```{r}
set.seed(1616)
rf_tuned <- randomForest(Winner ~ ., data = train_data_rf,
                         ntree = 1000,          # Increase trees
                         mtry = floor(sqrt(length(top_features))),  # Optimize variable selection per split
                         nodesize = 5,          # Minimum node size for splits
                         importance = TRUE)

# View summary
print(rf_tuned)

```

```{r}
# Predict on test data
test_data_rf$Predicted <- predict(rf_tuned, newdata = test_data_rf, type = "class")

# Confusion matrix
conf_matrix <- table(test_data_rf$Predicted, test_data_rf$Winner)
print(conf_matrix)

# Compute accuracy
accuracy <- sum(diag(conf_matrix)) / sum(conf_matrix)
print(paste("Accuracy:", round(accuracy * 100, 2), "%"))

```
### Drop unimportant features 

```{r}
important_features <- c("Winner", "RedAge", "BlueAge", "TotalFightTimeSecs", 
                        "RedDecOdds", "BlueDecOdds", "RSubOdds", "BSubOdds", 
                        "RKOOdds", "BKOOdds", "Log_EVRatio", "Finish", "FinishRound",
                        "ReachDif", "WeightClass", "BlueAvgSigStrPct", "RedAvgSigStrPct")

ufc_rf_data <- ufc_rf_data %>% select(all_of(important_features))

```

Find best model

```{r}

set.seed(1616)  # Ensure reproducibility

# Define hyperparameter grid
tune_grid <- expand.grid(mtry = c(2, 4, 6, 8),  # Number of predictors per split
                         nodesize = c(1, 5, 10),  # Minimum data points per leaf node
                         ntree = c(200, 500, 1000))  # Number of trees

# Store best model
best_model <- NULL
best_accuracy <- 0

# Tune Random Forest
for (i in 1:nrow(tune_grid)) {
  
  model <- randomForest(Winner ~ ., 
                        data = ufc_rf_data, 
                        ntree = tune_grid$ntree[i], 
                        mtry = tune_grid$mtry[i], 
                        nodesize = tune_grid$nodesize[i],
                        importance = TRUE)
  
  # OOB Error
  oob_error <- model$err.rate[nrow(model$err.rate), "OOB"]
  accuracy <- 1 - oob_error  # Convert error to accuracy
  
  print(paste("ntree:", tune_grid$ntree[i], "mtry:", tune_grid$mtry[i], 
              "nodesize:", tune_grid$nodesize[i], "OOB Accuracy:", round(accuracy, 4)))
  
  # Save best model
  if (accuracy > best_accuracy) {
    best_accuracy <- accuracy
    best_model <- model
  }
}

# Best model output
print(best_model)

```

```{r}
# Load Data
ufc_rf_data <- readRDS("ufc_men_imp.rds")

# Drop Non-Informative Columns
ufc_rf_data <- ufc_rf_data %>% select(-Date, -Location, -Country)

# Ensure Winner is a Factor
ufc_rf_data$Winner <- as.factor(ufc_rf_data$Winner)

# Set Seed
set.seed(1616)

# Train-Test Split (80-20)
train_index <- createDataPartition(ufc_rf_data$Winner, p = 0.8, list = FALSE)
train_data_rf <- ufc_rf_data[train_index, ]
test_data_rf <- ufc_rf_data[-train_index, ]
```



```{r}
best_rf <- randomForest(
  formula = Winner ~ ., 
  data = train_data_rf, 
  ntree = 500,        
  mtry = 2,           
  nodesize = 5,       
  importance = TRUE   
)

# Print Model Summary
print(best_rf)

# OOB Error Rate
oob_error <- best_rf$err.rate[500,1]
print(paste("OOB Error Rate:", round(oob_error * 100, 2), "%"))

```

```{r}
set.seed(1616)

# Define refined hyperparameter grid
tune_grid <- expand.grid(
  mtry = c(2, 3, 4),       # Try small values (smaller mtry tends to work better in RF)
  nodesize = c(3, 5, 7),   # Experiment with slightly smaller/larger nodesize
  ntree = c(500, 700, 1000) # Increase trees for better stability
)

# Initialize best accuracy tracker
best_accuracy <- 0
best_model <- NULL

# Loop over hyperparameter grid
for (i in 1:nrow(tune_grid)) {
  rf_model <- randomForest(
    formula = Winner ~ ., 
    data = train_data_rf, 
    ntree = tune_grid$ntree[i], 
    mtry = tune_grid$mtry[i], 
    nodesize = tune_grid$nodesize[i], 
    importance = TRUE
  )

  # Get OOB Accuracy
  oob_accuracy <- 1 - rf_model$err.rate[tune_grid$ntree[i], 1]

  # Track best model
  if (oob_accuracy > best_accuracy) {
    best_accuracy <- oob_accuracy
    best_model <- rf_model
  }

  print(paste("ntree:", tune_grid$ntree[i], "mtry:", tune_grid$mtry[i], "nodesize:", tune_grid$nodesize[i], "OOB Accuracy:", round(oob_accuracy * 100, 2), "%"))
}

# Best Model Summary
print(best_model)
print(paste("Best OOB Accuracy:", round(best_accuracy * 100, 2), "%"))

```


```{r}
set.seed(1616)
test_predictions <- predict(best_model, newdata = test_data_rf)
confusion_matrix <- table(test_predictions, test_data_rf$Winner)
print(confusion_matrix)
final_test_accuracy <- sum(diag(confusion_matrix)) / sum(confusion_matrix)
print(paste("Final Test Accuracy:", round(final_test_accuracy * 100, 2), "%"))


```



